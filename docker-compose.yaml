services:
  # From previous blog post. Here for your convenience ;)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
    - 11434:11434
    volumes:
      - ollama:/root/.ollama
    tty: true
    # If you have an Nvidia GPU, define this section, otherwise remove it to use
    # your CPU
    runtime: nvidia
    restart: unless-stopped
  
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8062:8080
    environment:
      - /ollama/api=http://ollama:11434/api
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_WEB_SEARCH_ENGINE=searxng
      - RAG_WEB_SEARCH_RESULT_COUNT=3
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
      - SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
  # SearXNG instance
  searxng:
    container_name: searxng
    image: searxng/searxng:latest
    ports:
      - "8214:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    env_file:
      - .env
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
volumes:
  ollama: {}
  open-webui: {}